{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffcc0942",
   "metadata": {},
   "outputs": [],
   "source": [
    "##How to perform active machine learning\n",
    "\n",
    "# Normal libraries\n",
    "\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# Library to build a model using keras that is build on tensorflow\n",
    "# (a library on how to handle tensor and their function)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Library to build generic models\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "# Libraries about the learning process of the actual AI\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Useful libraries\n",
    "from matplotlib import pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6ff6018",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors = pd.read_csv(\"../../../Downloads/appa2_timeline.csv\", sep=\",\")\n",
    "Sensors.set_index(\"channel\", inplace=True)\n",
    "Sensors = Sensors.applymap(lambda x: x.replace(\"*\", \"\"))\n",
    "\n",
    "df = read_csv(\"../../DataSet_FBK/ViaBolz.csv\", sep=\",\", parse_dates=[\"ts\"])\n",
    "df.set_index(\"ts\", inplace=True)\n",
    "\n",
    "change = {}\n",
    "for i in range(1, 9):\n",
    "    change[f\"S{str(i)}_R1\"] = f\"T{str(i)}\"\n",
    "    change[f\"S{str(i)}_R2\"] = f\"R{str(i)}\"\n",
    "    change[f\"S{str(i)}_Voltage\"] = f\"V{str(i)}\"\n",
    "df = df.rename(columns=change)\n",
    "\n",
    "\n",
    "Test = {}\n",
    "maxim = len(Sensors.columns)\n",
    "for i, col in enumerate(Sensors.columns):\n",
    "    materials = []\n",
    "    if i == maxim - 1:\n",
    "        for j, material in enumerate(Sensors[col]):\n",
    "            f = materials.count(material) + 1\n",
    "            materials.append(material)\n",
    "            if list(Test.keys()).count(material + str(f)) == 1:\n",
    "                Test[material + str(f)] = pd.concat(\n",
    "                    [\n",
    "                        Test[material + str(f)],\n",
    "                        df[col:][\n",
    "                            [\"R\" + str(j + 1), \"T\" + str(j + 1), \"V\" + str(j + 1)]\n",
    "                        ].rename(\n",
    "                            columns={\n",
    "                                \"R\" + str(j + 1): \"R\",\n",
    "                                \"T\" + str(j + 1): \"HT\",\n",
    "                                \"V\" + str(j + 1): \"V\",\n",
    "                            }\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                Test[material + str(f)] = df[col:][\n",
    "                    [\"R\" + str(j + 1), \"T\" + str(j + 1), \"V\" + str(j + 1)]\n",
    "                ].rename(\n",
    "                    columns={\n",
    "                        \"R\" + str(j + 1): \"R\",\n",
    "                        \"T\" + str(j + 1): \"HT\",\n",
    "                        \"V\" + str(j + 1): \"V\",\n",
    "                    }\n",
    "                )\n",
    "    else:\n",
    "        for j, material in enumerate(Sensors[col]):\n",
    "            f = materials.count(material) + 1\n",
    "            materials.append(material)\n",
    "            if list(Test.keys()).count(material + str(f)) == 1:\n",
    "                Test[material + str(f)] = pd.concat(\n",
    "                    [\n",
    "                        Test[material + str(f)],\n",
    "                        df[col : Sensors.columns[i + 1]][\n",
    "                            [\"R\" + str(j + 1), \"T\" + str(j + 1), \"V\" + str(j + 1)]\n",
    "                        ].rename(\n",
    "                            columns={\n",
    "                                \"R\" + str(j + 1): \"R\",\n",
    "                                \"T\" + str(j + 1): \"HT\",\n",
    "                                \"V\" + str(j + 1): \"V\",\n",
    "                            }\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                Test[material + str(f)] = df[col : Sensors.columns[i + 1]][\n",
    "                    [\"R\" + str(j + 1), \"T\" + str(j + 1), \"V\" + str(j + 1)]\n",
    "                ].rename(\n",
    "                    columns={\n",
    "                        \"R\" + str(j + 1): \"R\",\n",
    "                        \"T\" + str(j + 1): \"HT\",\n",
    "                        \"V\" + str(j + 1): \"V\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "df8 = Test[\"LaFeO31\"]\n",
    "\n",
    "df_appa = read_csv(\"../../DataSet_APPA/APPA2.csv\")\n",
    "df_appa\n",
    "df_appa.Time = pd.to_datetime(df_appa.Time)\n",
    "timezone_offset = datetime.timedelta(hours=2)\n",
    "timezone = datetime.timezone(timezone_offset)\n",
    "\n",
    "df_appa.Time = df_appa.Time.apply(lambda x: x.replace(tzinfo=timezone))\n",
    "df_appa.rename(columns = {\"Time\":\"ts\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb5fb11b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'Time'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15568\\3077312442.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf_appa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_appa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_appa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtimezone_offset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhours\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtimezone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimezone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimezone_offset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5985\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5986\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5987\u001b[0m         ):\n\u001b[0;32m   5988\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5989\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'Time'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df8 = df8.resample(\"1H\").mean()\n",
    "df_tot = df8.reset_index().merge(df_appa)\n",
    "df_tot.set_index(\"ts\",inplace=True)\n",
    "df_tot = df_tot.sort_index().dropna(how=\"all\")\n",
    "df_tot = df_tot.dropna(how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25bf77d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5e0f9bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 16\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m a \u001b[39m*\u001b[39m Sig\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mb \u001b[39m+\u001b[39m d \u001b[39m*\u001b[39m T \u001b[39m+\u001b[39m e \u001b[39m+\u001b[39m c \u001b[39m*\u001b[39m AH\n\u001b[0;32m     10\u001b[0m \u001b[39m##df8 is an example of a dataframe, indexed with data and with columns name not really informative\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[39m##You are training here from a start to end the slicing of a database\u001b[39;00m\n\u001b[0;32m     13\u001b[0m (\n\u001b[0;32m     14\u001b[0m     popt,\n\u001b[0;32m     15\u001b[0m     pcov,\n\u001b[1;32m---> 16\u001b[0m ) \u001b[39m=\u001b[39m curve_fit(  \u001b[39m# popt are the optimal values, pcov is the covariance matrix of the optimized values.\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m     f\u001b[39m=\u001b[39;49mf_model,  \u001b[39m# model function\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m     xdata\u001b[39m=\u001b[39;49m(\n\u001b[0;32m     19\u001b[0m         df8\u001b[39m.\u001b[39;49msort_index()\u001b[39m.\u001b[39;49mloc[\u001b[39m\"\u001b[39;49m\u001b[39m2021-01-01\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m\"\u001b[39;49m\u001b[39m2021-02-1\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mV\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues,\n\u001b[0;32m     20\u001b[0m         df8\u001b[39m.\u001b[39;49msort_index()\u001b[39m.\u001b[39;49mloc[\u001b[39m\"\u001b[39;49m\u001b[39m2021-01-01\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m\"\u001b[39;49m\u001b[39m2021-02-1\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mR\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues,\n\u001b[0;32m     21\u001b[0m         df8\u001b[39m.\u001b[39;49msort_index()\u001b[39m.\u001b[39;49mloc[\u001b[39m\"\u001b[39;49m\u001b[39m2021-01-01\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m\"\u001b[39;49m\u001b[39m2021-02-1\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mHT\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues,\n\u001b[0;32m     22\u001b[0m     ),  \u001b[39m# x data\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m     ydata\u001b[39m=\u001b[39;49mdf_tot[\u001b[39m\"\u001b[39;49m\u001b[39m2021-01-01\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m\"\u001b[39;49m\u001b[39m2022-01-1\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mPM10\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues,  \u001b[39m# y data\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m     p0\u001b[39m=\u001b[39;49m(\u001b[39m4\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m),  \u001b[39m# initial value of the parameters\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m     maxfev\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m,  \u001b[39m# Number of times in which you repeat the training, close to epochs\u001b[39;49;00m\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     27\u001b[0m a_opt, b_opt, c_opt, d_opt, e_opt, f_opt, g_opt \u001b[39m=\u001b[39m popt\n\u001b[0;32m     28\u001b[0m df8[\u001b[39m\"\u001b[39m\u001b[39mmodel 1\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m f_model(\n\u001b[0;32m     29\u001b[0m     (df8[\u001b[39m\"\u001b[39m\u001b[39mV\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues, df8[\u001b[39m\"\u001b[39m\u001b[39mR\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues, df8[\u001b[39m\"\u001b[39m\u001b[39mHT\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues),\n\u001b[0;32m     30\u001b[0m     a_opt,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     g_opt,\n\u001b[0;32m     37\u001b[0m )  \u001b[39m# Here you are predicting the value of your complete samples\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_minpack_py.py:888\u001b[0m, in \u001b[0;36mcurve_fit\u001b[1;34m(f, xdata, ydata, p0, sigma, absolute_sigma, check_finite, bounds, method, jac, full_output, nan_policy, **kwargs)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[39m# optimization may produce garbage for float32 inputs, cast them to float64\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[39mif\u001b[39;00m check_finite:\n\u001b[1;32m--> 888\u001b[0m     ydata \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray_chkfinite(ydata, \u001b[39mfloat\u001b[39;49m)\n\u001b[0;32m    889\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    890\u001b[0m     ydata \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(ydata, \u001b[39mfloat\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lucag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:627\u001b[0m, in \u001b[0;36masarray_chkfinite\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    625\u001b[0m a \u001b[39m=\u001b[39m asarray(a, dtype\u001b[39m=\u001b[39mdtype, order\u001b[39m=\u001b[39morder)\n\u001b[0;32m    626\u001b[0m \u001b[39mif\u001b[39;00m a\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mchar \u001b[39min\u001b[39;00m typecodes[\u001b[39m'\u001b[39m\u001b[39mAllFloat\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misfinite(a)\u001b[39m.\u001b[39mall():\n\u001b[1;32m--> 627\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    628\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39marray must not contain infs or NaNs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    629\u001b[0m \u001b[39mreturn\u001b[39;00m a\n",
      "\u001b[1;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "##Example of curve fit\n",
    "\n",
    "\n",
    "##Define a model\n",
    "def f_model(x, a, b, c, d, e):\n",
    "    Sig, T, RH, AH = x\n",
    "    return a * Sig**b + d * T + e + c * AH\n",
    "\n",
    "\n",
    "##df8 is an example of a dataframe, indexed with data and with columns name not really informative\n",
    "\n",
    "##You are training here from a start to end the slicing of a database\n",
    "(\n",
    "    popt,\n",
    "    pcov,\n",
    ") = curve_fit(  # popt are the optimal values, pcov is the covariance matrix of the optimized values.\n",
    "    f=f_model,  # model function\n",
    "    xdata=(\n",
    "        df8.sort_index().loc[\"2021-01-01\":\"2021-02-1\"][\"V\"].values,\n",
    "        df8.sort_index().loc[\"2021-01-01\":\"2021-02-1\"][\"R\"].values,\n",
    "        df8.sort_index().loc[\"2021-01-01\":\"2021-02-1\"][\"HT\"].values,\n",
    "    ),  # x data\n",
    "    ydata=df_tot[\"2021-01-01\":\"2022-01-1\"][\"PM10\"].values,  # y data\n",
    "    p0=(4, 1, 1, 1, 1, 1, 1),  # initial value of the parameters\n",
    "    maxfev=10000,  # Number of times in which you repeat the training, close to epochs\n",
    ")\n",
    "a_opt, b_opt, c_opt, d_opt, e_opt, f_opt, g_opt = popt\n",
    "df8[\"model 1\"] = f_model(\n",
    "    (df8[\"V\"].values, df8[\"R\"].values, df8[\"HT\"].values),\n",
    "    a_opt,\n",
    "    b_opt,\n",
    "    c_opt,\n",
    "    d_opt,\n",
    "    e_opt,\n",
    "    f_opt,\n",
    "    g_opt,\n",
    ")  # Here you are predicting the value of your complete samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8119a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Example of Scaler, matrices or something like these\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_square_error\n",
    "\n",
    "##Place whatever dataframe you want\n",
    "X = df8.drop([#Place here the values you want to predict], axis = 1)\n",
    "\n",
    "y = df8[[#Place here the values you want to predict]].copy()\n",
    "\n",
    "INPUT_DIM = len(X.columns())\n",
    "OUTPUT_DIM = len(y.columns())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39988dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and test, remember that you may want to not shuffle data while dividing them\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, shuffle=True\n",
    ")\n",
    "# You perform a standard scaler (or minmax scaler) to make the data more usable by the machine learning\n",
    "# model\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(y_train)\n",
    "\n",
    "y_train_s = scaler.transform(y_train)\n",
    "y_test_s = pd.DataFrame(data=scaler.transform(y_test), columns=y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04943e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "# Experiment with deeper and wider networks\n",
    "model = Sequential()\n",
    "# By Sequential it means that the layers are connected one after the one before so that you have\n",
    "# a defined flow of information\n",
    "\n",
    "\n",
    "# Input layer, Dense layer meaning that the layer is connected to all the neurons to the layer before\n",
    "# kernel_initializer is the how the starting values of the parameters of your neural network (NN)\n",
    "# input_dim is the number of features of the dataset that you are feeding to your NN\n",
    "# activation function is how you introduce non linearity in your model.\n",
    "\n",
    "model.add(\n",
    "    Dense(64, kernel_initializer=\"normal\", input_dim=INPUT_DIM, activation=\"selu\")\n",
    ")\n",
    "\n",
    "# Hidden layers:\n",
    "# Dropout layer is just a layer where you drops, for example the 20% of the links between the two layers\n",
    "# that it is placed between\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(128, kernel_initializer=\"normal\", activation=\"selu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, kernel_initializer=\"normal\", activation=\"selu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(16, kernel_initializer=\"normal\", activation=\"selu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(OUTPUT_DIM, kernel_initializer=\"normal\", activation=\"linear\"))\n",
    "\n",
    "# with add you can add a new layer, with pop you remove the last layer you have placed there\n",
    "\n",
    "\n",
    "# You are compiling your model, so from now on you can't change its structure. Here you specify its loss function,\n",
    "# the function that you want to minimize, the optimizer that is how you want to optimize the parameters,\n",
    "# the metrics that is another value that you can use to measure your model but you won't use it to optimize\n",
    "# the model\n",
    "model.compile(loss=a2, optimizer=\"adam\", metrics=[a2])\n",
    "# You are showing how your NN is formed\n",
    "model.summary()\n",
    "\n",
    "##Here you are defining something that is not necessary. You are defining condition by which you are saving\n",
    "# definite models. For example in that case you are specifing where do you want to place them, you are searching\n",
    "# the model where the value of the loss function over the validation set (data not seen by the model during\n",
    "# the training) is minimum. The weight of the \"best model\" by that value is saved there.\n",
    "checkpoint_name = \"./tmp/checkpoint\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    checkpoint_name, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"min\"\n",
    ")\n",
    "callbacks_list = [checkpoint]\n",
    "# In the history you collect the important information that happens during the fit, value of loss function,\n",
    "# metrics and so on in one variable.\n",
    "# During the fit  the parameters of the NN are updated. You are specifing the training data, X and y, and either\n",
    "# you give the fit function the validation data or you give them a percentage as the validation_split, in this case 20%\n",
    "# after that you are specifing the number of epochs, meaning how many time the model has seen the same data,\n",
    "# and then the callbacks that you want to use. Notice that you can have more than one callback.\n",
    "# There are more parameters that you can implement, so take a look at them\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_s, validation_split=0.2, epochs=300, callbacks=[checkpoint]\n",
    ")\n",
    "# After performing the training you are uploading the best model that you have saved in the callback, updating\n",
    "# the value of your parameters\n",
    "model.load_weights(checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bee7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, \"y\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "acc = history.history[\"mean_squared_error\"]\n",
    "val_acc = history.history[\"val_mean_squared_error\"]\n",
    "plt.plot(epochs, acc, \"y\", label=\"Training MAE\")\n",
    "plt.plot(epochs, val_acc, \"r\", label=\"Validation MAE\")\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "############################################\n",
    "# Predict on test data\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "y_test_ssaved = y_test_s\n",
    "y_test_s = y_test\n",
    "print(\"Predicted values are: \", predictions)\n",
    "print(\"Real values are: \", y_test_s)\n",
    "\n",
    "##############################################\n",
    "# Plot and compare prediction and real value\n",
    "t = np.arange(0, predictions.size, 1)\n",
    "plt.scatter(predictions, y_test_s, label=\"Prediction\")\n",
    "plt.scatter(y_test_s, y_test_s, label=\"Real Value\")\n",
    "plt.title(\"Prediction\")\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"Real Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf906e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Example of RandomForestRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9ffc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Same thing you have done before, define an X and y from a dataset if you want to try.\n",
    "\n",
    "xtrain, xtest, ytrain, ytest=train_test_split(X, Y, shuffle = True, test_size=0.20)\n",
    "scaler=MinMaxScaler()\n",
    "scaler2 = MinMaxScaler()\n",
    "scaler.fit(xtrain)\n",
    "\n",
    "X_train = scaler.transform(xtrain)\n",
    "X_test = scaler.transform(xtest)\n",
    "scaler2.fit(ytrain.values.reshape(-1, 1))\n",
    "\n",
    "y_train = scaler2.transform(ytrain.values.reshape(-1, 1))\n",
    "y_test = scaler2.transform(ytest.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85af19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor()\n",
    "\n",
    " \n",
    "#Here are some hyperparameter of your regressor, it's a bit better and easier to look up at what the difference\n",
    "#parameter does. All of this hyperparameter can be optimized using the validation data.\n",
    "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
    "                      max_depth=10, max_features=3, max_leaf_nodes=None,\n",
    "                      max_samples=0.1, min_impurity_decrease=0.0,\n",
    "                      min_samples_leaf=0.01,\n",
    "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
    "                      random_state=None, verbose=0, warm_start=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89954853",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit of the RandomForestRegressor\n",
    "\n",
    "rfr.fit(X_train, y_train.reshape(-1, 1))\n",
    "#evaluation of its performance by the R-squared\n",
    "\n",
    "score = rfr.score(X_train, y_train.reshape(-1, 1))\n",
    "print(\"R-squared:\", score) \n",
    "\n",
    "ypred = rfr.predict(X_test)\n",
    "#evaluation of its performance by the mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y_test, ypred)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", mse*(1/2.0)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
